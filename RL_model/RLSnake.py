from RL_utils import get_train_decision, calculate_map_state, update_graph_state
from GameGraph import GameGraph
from model import RlModel
from Config import ConfigSet, set_seed
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
set_seed(42)


class RLSnake:
    def __init__(self, config):
        # è®¾ç½®ðŸä»Žåœ°å›¾ä¸­é—´å¼€å§‹ï¼Œå…¶ä¸­åœ°å›¾çš„ä¿¡æ¯ä¿å­˜åœ¨configè¿™ä¸ªdiskä¸­
        self.config = config
        self.start_point = config['start_point']
        self.graph = GameGraph(self.start_point, config['start_food'], config['edges'], config["size"])
        self.model = RlModel(config)
        self.choice = config['decision']
        self.bad_score = -10
        self.warn_score = -1
        self.correct_score = 1
        self.win_score = 10
        self.loss_function = nn.CrossEntropyLoss()
        self.lr = config['lr']
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)

    def get_step_value(self):
        # æ ¹æ®ç›®å‰çš„çŠ¶æ€ï¼Œè®¡ç®—å„ä¸ªchoiceæ‰€èŽ·å¾—çš„ç›¸åº”çš„value
        # å‡ ä¸ªæ ‡å‡†
        # ç¬¬ä¸€ï¼Œå¦‚æžœæ’žåˆ°è¾¹ç•Œæˆ–è€…è›‡èº«ä½“ï¼Œåˆ™ä¼šäº§ç”Ÿä¸€ä¸ªè´Ÿå€¼
        # ç¬¬äºŒï¼Œå¦‚æžœäº§ç”Ÿä¸€ä¸ªå¾ªçŽ¯ï¼Œé‚£ä¹ˆä¼šäº§ç”Ÿä¸€ä¸ªè´Ÿå€¼
        # ç¬¬ä¸‰ï¼Œå¦‚æžœåƒåˆ°ç³–ï¼Œäº§ç”Ÿä¸€ä¸ªæ­£å€¼
        # ç¬¬å››ï¼Œå¦‚æžœå‘ç³–èµ°ï¼Œäº§ç”Ÿä¸€ä¸ªè¾ƒå°çš„æ­£å€¼
        snake_head = self.graph.snake[-1]
        snake_tail = self.graph.snake[0]
        value_list = []
        for each in self.choice:
            next_station = (snake_head[0] + each[0], snake_head[1] + each[1])
            value = 0
            if not self.graph.is_valid_move(next_station):
                value += self.bad_score
            if next_station[0] == snake_tail[0] and next_station[1] == snake_tail[1]:  # åˆ¤æ–­ç§»åŠ¨çš„æ–¹å‘åˆšå¥½æ˜¯è›‡çš„å°¾éƒ¨ï¼Œå¢žåŠ ä¸€ä¸ªå°è´Ÿé¡¹ï¼Œé˜²æ­¢å‡ºçŽ°å¾ªçŽ¯
                value += self.warn_score
            if each[0] * (self.graph.food_x - snake_head[0]) > 0 or each[1] * (self.graph.food_y - snake_head[1]) > 0:
                value += self.correct_score
            if next_station[0] == self.graph.food_x and next_station[1] == self.graph.food_y:
                value += self.win_score
            value_list.append(value)
        return value_list

    def train_run(self, max_step=10000):
        # è®¾ç½®è®­ç»ƒçš„æœ€å¤§æ­¥æ•°ï¼Œé˜²æ­¢å‡ºçŽ°å¾ªçŽ¯
        for i in tqdm(range(max_step), total=max_step):
            step_value = self.get_step_value()
            next_decision = get_train_decision(self.config, step_value)

            # ä¸­é—´è®­ç»ƒæ¨¡åž‹

            self.graph = update_graph_state(self.graph, next_decision)



