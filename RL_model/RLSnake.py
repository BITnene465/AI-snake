from RL_utils import get_train_decision, calculate_map_state, update_graph_state, generate_new_candy
from GameGraph import GameGraph
from model import RlModel
from Config import ConfigSet, set_seed
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm
set_seed(42)

device = "cuda" if torch.cuda.is_available() else "cpu"


class RLSnake:
    def __init__(self, config):
        # è®¾ç½®ğŸä»åœ°å›¾ä¸­é—´å¼€å§‹ï¼Œå…¶ä¸­åœ°å›¾çš„ä¿¡æ¯ä¿å­˜åœ¨configè¿™ä¸ªdiskä¸­
        self.config = config
        self.start_point = config['start_point']
        self.graph = GameGraph(self.start_point, config['start_food'], config['edges'], config["size"])
        self.model = RlModel(config).to(device)
        self.choice = config['decision']
        self.gamma = config['gamma']
        self.bad_score = -10
        self.warn_score = -1
        self.correct_score = 1
        self.win_score = 10
        self.loss_function = nn.MSELoss()
        self.lr = config['lr']
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.state_list = []  # ç”¨äºåœ¨è®­ç»ƒä¸­è®°å½•è¿‡ç¨‹çš„state
        # å°†configè®¾ç½®ä¸ºå¦‚ä¸‹çš„decision
        # self.decision = [[0, 1], [0, -1], [1, 0], [-1, 0]]

    def get_step_value(self, graph):
        # æ ¹æ®ç›®å‰çš„çŠ¶æ€ï¼Œè®¡ç®—å„ä¸ªchoiceæ‰€è·å¾—çš„ç›¸åº”çš„value
        # å‡ ä¸ªæ ‡å‡†
        # ç¬¬ä¸€ï¼Œå¦‚æœæ’åˆ°è¾¹ç•Œæˆ–è€…è›‡èº«ä½“ï¼Œåˆ™ä¼šäº§ç”Ÿä¸€ä¸ªè´Ÿå€¼
        # ç¬¬äºŒï¼Œå¦‚æœäº§ç”Ÿä¸€ä¸ªå¾ªç¯ï¼Œé‚£ä¹ˆä¼šäº§ç”Ÿä¸€ä¸ªè´Ÿå€¼
        # ç¬¬ä¸‰ï¼Œå¦‚æœåƒåˆ°ç³–ï¼Œäº§ç”Ÿä¸€ä¸ªæ­£å€¼
        # ç¬¬å››ï¼Œå¦‚æœå‘ç³–èµ°ï¼Œäº§ç”Ÿä¸€ä¸ªè¾ƒå°çš„æ­£å€¼
        snake_head = graph.snake[-1]
        snake_tail = graph.snake[0]
        value_list = []
        for each in self.choice[graph.head_index]:
            next_station = (snake_head[0] + each[0], snake_head[1] + each[1])
            value = 0
            if not self.graph.is_valid_move(each):
                value += self.bad_score
            if next_station[0] == snake_tail[0] and next_station[1] == snake_tail[1]:  # åˆ¤æ–­ç§»åŠ¨çš„æ–¹å‘åˆšå¥½æ˜¯è›‡çš„å°¾éƒ¨ï¼Œå¢åŠ ä¸€ä¸ªå°è´Ÿé¡¹ï¼Œé˜²æ­¢å‡ºç°å¾ªç¯
                value += self.warn_score
            if each[0] * (graph.food_x - snake_head[0]) > 0 or each[1] * (graph.food_y - snake_head[1]) > 0:
                value += self.correct_score
            if next_station[0] == graph.food_x and next_station[1] == graph.food_y:
                value += self.win_score
            value_list.append(value)
        return value_list

    def train_run(self, config, max_step=10000):
        # è®¾ç½®è®­ç»ƒçš„æœ€å¤§æ­¥æ•°ï¼Œé˜²æ­¢å‡ºç°å¾ªç¯
        # é¦–å…ˆå°è¯•ä½¿ç”¨DQN
        # æ¯ä¸€è½®å¼€å§‹åˆå§‹åŒ–graph
        graph = GameGraph(snake=config['start_point'], food=config['start_food'],
                          edges=config['edges'], square_size=config["size"])
        graph = generate_new_candy(graph)
        for _ in range(max_step):
            self.optimizer.zero_grad()
            step_value = torch.tensor(self.get_step_value(graph), device=device)
            # è·å–å½“å‰çš„graphçš„çŠ¶æ€ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªä¸€ç»´å‘é‡
            graph_station = calculate_map_state(graph, choices=self.choice[graph.head_index]).view(-1).to(device)
            next_step_q_value = self.model(graph_station)
            _, max_id = torch.max(next_step_q_value, dim=-1)
            max_q = next_step_q_value[max_id]
            loss_list = []
            for j in range(len(step_value)):
                step_loss = self.loss_function(next_step_q_value[j], (step_value[j] + self.gamma * max_q))
                loss_list.append(step_loss)
            loss = sum(loss_list)
            loss.backward()
            self.optimizer.step()
            next_decision, decision_index = get_train_decision(config, graph.head_index,
                                                               next_step_q_value.detach().cpu())
            if graph.is_valid_move(next_decision):
                graph = update_graph_state(graph, decision_index, next_decision)
                continue
            else:
                self.state_list.append(len(graph.snake))
                break

    def run_test(self, graph=None):
        if graph is None:
            graph = GameGraph(self.start_point, self.config['start_food'], self.config['edges'], self.config["size"])
        model_state = torch.load(self.config['save_path'])
        self.model.load_state_dict(model_state)
        with torch.no_grad():
            while True:
                graph_station = torch.tensor(calculate_map_state(graph), device=device)
                next_step_q_value = self.model(graph_station)
                next_decision = self.choice[next_step_q_value.argmax(dim=-1)]
                graph, is_continue = update_graph_state(graph, next_decision)
                if not is_continue:
                    break


if __name__ == "__main__":
    config_ = ConfigSet(edges={'xmin': 0, 'xmax': 11, 'ymin': 0, 'ymax': 11},
                        start_point=[(6, 6)],
                        start_food=[1, 1],
                        size=24,
                        decision=[[(0, -1), (-1, 0), (1, 0)],
                                  [(1, 0), (0, -1), (0, 1)],
                                  [(0, 1), (1, 0), (-1, 0)],
                                  [(-1, 0), (0, 1), (0, -1)]],
                        gamma=0.8,
                        lr=0.0005,
                        input_layer=[11, 256],
                        hidden_layer=[256, 3],
                        save_path=r"G:/python_program/AI_snake/AI-snake-main/RL_model/model_param")
    RL = RLSnake(config_)
    for t in tqdm(range(100), total=100):
        RL.train_run(config_)
    torch.save(RL.model.state_dict(), r"G:\python_program\AI_snake\AI-snake-main\RL_model\model_param\model.pt")
    plt.plot([i for i in range(len(RL.state_list))], RL.state_list)
    plt.show()
    # å¤´çš„æœå‘åˆ†åˆ«ä¸ºå·¦ï¼Œä¸Šï¼Œå³ï¼Œä¸‹, ç§»åŠ¨æ–¹å‘åˆ†åˆ«ä¸ºforwardï¼Œturn leftï¼Œ turn right


